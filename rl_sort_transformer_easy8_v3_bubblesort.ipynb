{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ba0d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import math\n",
    "import random\n",
    "\n",
    "MIN_LIST_LEN = 8\n",
    "MAX_LIST_LEN = 8\n",
    "MAX_STEPS = 210\n",
    "\n",
    "SUCCESS_REWARD = 0.5\n",
    "STEP_REWARD = -0.3\n",
    "COMPARISON_ENTROPY_MULTIPLIER = -0.00\n",
    "SWAP_REWARD = 1.0\n",
    "INVALID_ACTION_REWARD = -10.0\n",
    "LONGTERM_GAMMA = 0.99\n",
    "SHORTTERM_GAMMA = 0.7\n",
    "\n",
    "EPS_START = 0.5\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "LR_SCHEDULER_GAMMA = 0.93\n",
    "NUM_EPISODES = 100000\n",
    "EPISODES_SAVE = 1000\n",
    "OUTPUT_DIR = 'datasets/rl_sort_transformer_easy/list8_transformer4_192_gamma07_step210_v3_bubblesort'\n",
    "\n",
    "# Define the vocabulary\n",
    "vocab = {\n",
    "    'Comparison': 0,\n",
    "    'Swap': 1,\n",
    "    'less': 2,\n",
    "    'equal': 3,\n",
    "    'more': 4,\n",
    "    '0': 5,\n",
    "    '1': 6,\n",
    "    '2': 7,\n",
    "    '3': 8,\n",
    "    '4': 9,\n",
    "    '5': 10,\n",
    "    '6': 11,\n",
    "    '7': 12,\n",
    "    '8': 13,\n",
    "    '9': 14,\n",
    "    '10': 15,\n",
    "    '11': 16,\n",
    "    '12': 17,\n",
    "    '13': 18,\n",
    "    '14': 19,\n",
    "    '15': 20,\n",
    "    'len1': 21,\n",
    "    'len2': 22,\n",
    "    'len3': 23,\n",
    "    'len4': 24,\n",
    "    'len5': 25,\n",
    "    'len6': 26,\n",
    "    'len7': 27,\n",
    "    'len8': 28,\n",
    "    'len9': 29,\n",
    "    'len10': 30,\n",
    "    'len11': 31,\n",
    "    'len12': 32,\n",
    "    'len13': 33,\n",
    "    'len14': 34,\n",
    "    'len15': 35,\n",
    "    'len16': 36,\n",
    "}\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "def compute_entropy(N, alpha=1):\n",
    "    K = 2**N\n",
    "    values = np.arange(K)\n",
    "    unnormalized_probs = np.exp(-alpha * values)\n",
    "    Z = unnormalized_probs.sum()\n",
    "    probs = unnormalized_probs / Z\n",
    "    return values, -np.log2(probs)\n",
    "\n",
    "_, int_entropy = compute_entropy(4)\n",
    "\n",
    "def get_entropy_of_integer(x):\n",
    "    x = min(15, abs(x))\n",
    "    return int_entropy[x]\n",
    "\n",
    "def compute_min_delta_entropy(comparisons):\n",
    "    # Initialize the result list to store minDelta values\n",
    "    min_delta = None\n",
    "\n",
    "    # Iterate through each pair in the comparisons list\n",
    "    i = len(comparisons) - 1\n",
    "    xi, yi = comparisons[i]\n",
    "    if i == 0:\n",
    "        # For i = 0, use the first case directly\n",
    "        min_delta = (xi, min(yi, yi - xi), 0)\n",
    "    else:\n",
    "        # For i > 0, compute all possible options and select the minimal one\n",
    "        options = []\n",
    "\n",
    "        # Simple Entropy\n",
    "        simple_entropy = (xi, min(yi, yi - xi), 0)\n",
    "        options.append(simple_entropy)\n",
    "\n",
    "        # First Delta Entropy\n",
    "        xi_prev, yi_prev = comparisons[i - 1]\n",
    "        first_delta_entropy = (xi - xi_prev, yi - yi_prev, 0)\n",
    "        options.append(first_delta_entropy)\n",
    "\n",
    "        # Second Delta Entropy (only valid for i > 1)\n",
    "        if i > 1:\n",
    "            xi_prev2, yi_prev2 = comparisons[i - 2]\n",
    "            second_delta_entropy = (\n",
    "                (xi - xi_prev) - (xi_prev - xi_prev2),\n",
    "                (yi - yi_prev) - (yi_prev - yi_prev2),\n",
    "                0,\n",
    "            )\n",
    "            options.append(second_delta_entropy)\n",
    "\n",
    "        # Arbitrary Position Entropy (only valid for i > 1)\n",
    "        for j in range(i):\n",
    "            xj, yj = comparisons[j]\n",
    "            arbitrary_position_entropy = (\n",
    "                xi - xj,\n",
    "                yi - yj,\n",
    "                min(j, i - j),\n",
    "            )\n",
    "            options.append(arbitrary_position_entropy)\n",
    "\n",
    "        # Find the option with the minimal sum\n",
    "        min_delta = min(options, key=lambda t: sum([get_entropy_of_integer(x) for x in t]))\n",
    "\n",
    "    entropy = sum([get_entropy_of_integer(x) for x in min_delta])\n",
    "    if len(comparisons) == 1:\n",
    "        return 3 * entropy\n",
    "    else:\n",
    "        return entropy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the environment\n",
    "class SortingEnv:\n",
    "    def __init__(self):\n",
    "        self.max_steps = MAX_STEPS\n",
    "\n",
    "    def reset(self):\n",
    "        self.length = random.randint(MIN_LIST_LEN, MAX_LIST_LEN)\n",
    "        self.list = [random.randint(1, 100) for _ in range(self.length)]\n",
    "        while self.list == sorted(self.list):\n",
    "            self.list = [random.randint(1, 100) for _ in range(self.length)]\n",
    "        self.indices = None\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        initial_token = 'len{}'.format(self.length)\n",
    "        return vocab[initial_token], self.list.copy()\n",
    "    \n",
    "    def get_list(self):\n",
    "        return self.list\n",
    "    \n",
    "    def get_list_len(self):\n",
    "        return len(self.list)\n",
    "\n",
    "    def step(self, action_tokens):\n",
    "        action = action_tokens[0]\n",
    "        reward = -0.01  # default penalty\n",
    "        response_token = None\n",
    "\n",
    "        if action == vocab['Comparison']:\n",
    "            if len(action_tokens) != 3:\n",
    "                reward = INVALID_ACTION_REWARD\n",
    "                self.done = True\n",
    "                return response_token, reward, self.done, self.list.copy()\n",
    "            index1 = action_tokens[1] - vocab['0']\n",
    "            index2 = action_tokens[2] - vocab['0']\n",
    "            if index1 >= self.length or index2 >= self.length or index1 < 0 or index2 < 0:\n",
    "                reward = INVALID_ACTION_REWARD\n",
    "                self.done = True\n",
    "                return response_token, reward, self.done, self.list.copy()\n",
    "            self.indices = (index1, index2)\n",
    "            if self.list[index1] < self.list[index2]:\n",
    "                response_token = vocab['less']\n",
    "                reward = STEP_REWARD\n",
    "            elif self.list[index1] == self.list[index2]:\n",
    "                response_token = vocab['equal']\n",
    "                reward = STEP_REWARD * 2\n",
    "            else:\n",
    "                response_token = vocab['more']\n",
    "                reward = STEP_REWARD\n",
    "        elif action == vocab['Swap']:\n",
    "            if self.indices is None:\n",
    "                reward = INVALID_ACTION_REWARD\n",
    "                self.done = True\n",
    "                return response_token, reward, self.done, self.list.copy()\n",
    "            index1, index2 = self.indices\n",
    "            prev_list = self.list.copy()\n",
    "            self.list[index1], self.list[index2] = self.list[index2], self.list[index1]\n",
    "            if self.list == sorted(self.list):\n",
    "                reward = SUCCESS_REWARD\n",
    "                self.done = True\n",
    "            #elif prev_list[index1] > prev_list[index2] and self.list[index1] <= self.list[index2]:\n",
    "            #    reward = 0.1\n",
    "            elif (index1 < index2 and prev_list[index1] > prev_list[index2] and self.list[index1] <= self.list[index2]) or \\\n",
    "                (index1 > index2 and prev_list[index1] < prev_list[index2] and self.list[index1] >= self.list[index2]):\n",
    "                reward = SWAP_REWARD\n",
    "            elif (index1 < index2 and prev_list[index1] < prev_list[index2] and self.list[index1] >= self.list[index2]) or \\\n",
    "                (index1 > index2 and prev_list[index1] > prev_list[index2] and self.list[index1] <= self.list[index2]):\n",
    "                reward = -SWAP_REWARD\n",
    "            else:\n",
    "                reward = STEP_REWARD\n",
    "            self.indices = None\n",
    "        else:\n",
    "            reward = INVALID_ACTION_REWARD\n",
    "            self.done = True\n",
    "\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            self.done = True\n",
    "        return response_token, reward, self.done, self.list.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0cf9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding for Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "        pe = pe.unsqueeze(1)  # (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=192, nhead=8, num_layers=4):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.embedding.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "def decode(input_tokens, inv_vocab):\n",
    "    return ' '.join([inv_vocab[x] for x in input_tokens])\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, episode, folder, filename):\n",
    "    \"\"\"\n",
    "    Save the model and optimizer state to the designated filepath.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to save.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer whose state to save.\n",
    "        episode (int): The current episode number.\n",
    "        filepath (str): The path where to save the checkpoint.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    # Save the checkpoint\n",
    "    torch.save({\n",
    "        'episode': episode,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, filepath)\n",
    "    print(f\"Checkpoint saved at episode {episode} to {filepath}\")\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer):\n",
    "    \"\"\"\n",
    "    Load the model and optimizer state from the designated filepath.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path from where to load the checkpoint.\n",
    "        model (nn.Module): The model into which to load the state_dict.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer into which to load the state.\n",
    "\n",
    "    Returns:\n",
    "        int: The episode number to resume from.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        episode = checkpoint['episode']\n",
    "        print(f\"Checkpoint loaded from {filepath}, resuming from episode {episode}\")\n",
    "        return episode\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {filepath}, starting from scratch.\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a13cca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from datasets/rl_sort_transformer_easy/list8_transformer4_192_gamma07_step210_v3_bubblesort/ckpt_99000_0.9940_86.73.pth, resuming from episode 99000\n",
      "len8 Comparison 0 1 more Swap Comparison 1 2 more Swap Comparison 2 3 less Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 5 6 less Comparison 6 7 more Swap Comparison 0 1 more Swap Comparison 1 2 less Comparison 2 3 less Comparison 3 4 more Swap Comparison 4 5 less Comparison 5 6 more Swap Comparison 0 1 less Comparison 1 2 less Comparison 2 3 more Swap Comparison 3 4 less Comparison 4 5 more Swap Comparison 0 1 less Comparison 1 2 less Comparison 2 3 less Comparison 3 4 more Swap Comparison 0 1 less Comparison 1 2 less Comparison 0 1 less Comparison 2 3 more Swap Comparison 1 2 more Swap Comparison 1 2 less Comparison 0 1 more Swap\n",
      "Episode 0, loss:0.0065, succeed, steps:101, total reward:4.8000, 0.734266996383667 sec\n",
      "len8 Comparison 0 1 more Swap Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 less Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 6 7 more Swap Comparison 0 1 more Swap Comparison 1 2 more Swap Comparison 2 3 less Comparison 3 4 less Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 0 1 more Swap Comparison 1 2 less Comparison 2 3 less Comparison 3 4 less Comparison 4 5 more Swap\n",
      "Episode 1, loss:0.0052, succeed, steps:66, total reward:6.1000, 0.17763137817382812 sec\n",
      "len8 Comparison 0 1 more Swap Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 less Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 6 7 more Swap Comparison 0 1 less Comparison 1 2 less Comparison 2 3 less Comparison 3 4 equal Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 0 1 less Comparison 2 3 less Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 0 1 less Comparison 1 2 less Comparison 0 1 less Comparison 2 3 more Swap Comparison 3 4 more Swap\n",
      "Episode 2, loss:0.0034, succeed, steps:78, total reward:4.6000, 0.2003040313720703 sec\n",
      "len8 Comparison 0 1 more Swap Comparison 1 2 less Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 6 7 more Swap Comparison 0 1 less Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 less Comparison 5 6 more Swap Comparison 0 1 more Swap Comparison 2 3 more Swap Comparison 3 4 less Comparison 4 5 more Swap Comparison 0 1 less Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 0 1 more Swap Comparison 3 4 more Swap Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 0 1 less Comparison 0 1 less Comparison 1 2 more Swap Comparison 0 1 more Swap\n",
      "Episode 3, loss:0.0096, succeed, steps:105, total reward:12.1000, 0.27680397033691406 sec\n",
      "len8 Comparison 0 1 less Comparison 1 2 less Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 6 7 more Swap Comparison 0 1 less Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 0 1 less Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 0 1 less Comparison 2 3 more Swap Comparison 3 4 less Comparison 0 1 less Comparison 1 2 more Swap Comparison 1 2 less Comparison 2 3 more Swap Comparison 0 1 more Swap\n",
      "Episode 4, loss:0.0077, succeed, steps:96, total reward:9.7000, 0.2590515613555908 sec\n",
      "len8 Comparison 0 1 less Comparison 1 2 less Comparison 2 3 more Swap Comparison 3 4 less Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 6 7 less Comparison 0 1 less Comparison 1 2 more Swap Comparison 2 3 less Comparison 3 4 more Swap Comparison 4 5 less Comparison 5 6 less Comparison 0 1 less Comparison 1 2 less Comparison 2 3 more Swap Comparison 3 4 less Comparison 4 5 less Comparison 0 1 less Comparison 0 1 less Comparison 1 2 more Swap Comparison 2 3 less Comparison 3 4 less Comparison 0 1 more Swap\n",
      "Episode 5, loss:0.0031, succeed, steps:80, total reward:0.3000, 0.21321988105773926 sec\n",
      "len8 Comparison 0 1 less Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 6 7 less Comparison 0 1 more Swap Comparison 1 2 less Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 5 6 less Comparison 0 1 less Comparison 1 2 less Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 less Comparison 0 1 less Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 less Comparison 0 1 more Swap Comparison 2 3 less Comparison 0 1 less Comparison 1 2 more Swap\n",
      "Episode 6, loss:0.0051, succeed, steps:93, total reward:6.7000, 0.24499750137329102 sec\n",
      "len8 Comparison 0 1 less Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 6 7 more Swap Comparison 0 1 less Comparison 1 2 less Comparison 2 3 more Swap Comparison 3 4 less Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 0 1 less Comparison 1 2 more Swap Comparison 2 3 less Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 0 1 less Comparison 1 2 less Comparison 2 3 less Comparison 3 4 more Swap Comparison 0 1 less Comparison 1 2 less Comparison 2 3 more Swap Comparison 0 1 less Comparison 0 1 less Comparison 1 2 more Swap\n",
      "Episode 7, loss:0.0052, succeed, steps:99, total reward:6.1000, 0.2842705249786377 sec\n",
      "len8 Comparison 0 1 less Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 6 7 more Swap Comparison 0 1 more Swap Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 0 1 less Comparison 1 2 less Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 0 1 less Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 0 1 less Comparison 1 2 less Comparison 2 3 more Swap Comparison 0 1 less Comparison 1 2 more Swap Comparison 1 2 less Comparison 0 1 more Swap\n",
      "Episode 8, loss:0.0069, succeed, steps:104, total reward:11.1000, 0.2845163345336914 sec\n",
      "len8 Comparison 0 1 more Swap Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 6 7 less Comparison 0 1 more Swap Comparison 1 2 less Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 5 6 less Comparison 0 1 less Comparison 1 2 less Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 less Comparison 0 1 less Comparison 1 2 equal Comparison 2 3 more Swap Comparison 3 4 less Comparison 0 1 less Comparison 0 1 less Comparison 2 3 less Comparison 1 2 more Swap\n",
      "Episode 9, loss:0.0054, succeed, steps:92, total reward:5.4000, 0.2472217082977295 sec\n",
      "len8 Comparison 0 1 less Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 6 7 more Swap Comparison 0 1 more Swap Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 less Comparison 5 6 more Swap Comparison 0 1 less Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 4 5 more Swap Comparison 0 1 more Swap Comparison 2 3 less Comparison 3 4 more Swap Comparison 0 1 less Comparison 0 1 less Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 0 1 more Swap Comparison 1 2 more Swap Comparison 0 1 more Swap\n",
      "Episode 10, loss:0.0174, succeed, steps:102, total reward:12.4000, 0.32157230377197266 sec\n",
      "len8 Comparison 0 1 less Comparison 1 2 less Comparison 2 3 less Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 6 7 more Swap Comparison 0 1 less Comparison 1 2 less Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 less Comparison 5 6 more Swap Comparison 0 1 less Comparison 1 2 less Comparison 2 3 more Swap Comparison 3 4 less Comparison 4 5 more Swap Comparison 0 1 less Comparison 1 2 more Swap Comparison 2 3 less Comparison 3 4 more Swap Comparison 0 1 less Comparison 0 1 less Comparison 1 2 less Comparison 2 3 more Swap Comparison 1 2 more Swap Comparison 0 1 more Swap\n",
      "Episode 11, loss:0.0043, succeed, steps:98, total reward:5.1000, 0.26262402534484863 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len8 Comparison 0 1 more Swap Comparison 1 2 more Swap Comparison 2 3 less Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 6 7 more Swap Comparison 0 1 less Comparison 1 2 less Comparison 2 3 less Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 0 1 less Comparison 1 2 less Comparison 2 3 less Comparison 3 4 more Swap Comparison 4 5 more Swap\n",
      "Episode 12, loss:0.0048, succeed, steps:65, total reward:5.1000, 0.17924976348876953 sec\n",
      "len8 Comparison 0 1 more Swap Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 less Comparison 5 6 more Swap Comparison 6 7 more Swap Comparison 0 1 more Swap Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 less Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 0 1 less Comparison 2 3 less Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 1 2 less Comparison 0 1 less Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 1 2 less Comparison 0 1 less Comparison 0 1 less Comparison 1 2 less Comparison 0 1 less Comparison 1 2 less Comparison 2 3 more Swap Comparison 2 3 less Comparison 1 2 more Swap\n",
      "Episode 13, loss:0.6939, succeed, steps:107, total reward:7.5000, 0.27967071533203125 sec\n",
      "len8 Comparison 0 1 more Swap Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 5 6 less Comparison 6 7 more Swap Comparison 0 1 less Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 less Comparison 5 6 more Swap Comparison 0 1 less Comparison 2 3 more Swap Comparison 3 4 less Comparison 4 5 less Comparison 0 1 less Comparison 1 2 more Swap Comparison 1 2 less Comparison 2 3 less Comparison 3 4 less Comparison 0 1 more Swap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_bellman_returns(raw_rewards, gamma):\n",
    "    bellman_returns = []\n",
    "    R = 0\n",
    "    for r in raw_rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        bellman_returns.insert(0, R)\n",
    "    return bellman_returns\n",
    "\n",
    "# Training Loop\n",
    "def train(verbose=False):\n",
    "    # Removed torch.autograd.set_detect_anomaly(True)\n",
    "    vocab_size = len(vocab)\n",
    "    model = TransformerModel(vocab_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Reduced learning rate\n",
    "    scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=LR_SCHEDULER_GAMMA)\n",
    "    \n",
    "    # Optionally, load a checkpoint\n",
    "    #checkpoint_path = os.path.join(OUTPUT_DIR, \"ckpt_99000_0.9940_86.73.pth\")\n",
    "    #load_checkpoint(checkpoint_path, model, optimizer)\n",
    "\n",
    "    episode_cnt = 0\n",
    "    total_reward = 0.0\n",
    "    num_successes = 0\n",
    "    total_steps = 0\n",
    "    \n",
    "    for episode in range(NUM_EPISODES):\n",
    "        t1 = time.time()\n",
    "        model.train()  # Set model to training mode\n",
    "        env = SortingEnv()\n",
    "        initial_token_id, current_list = env.reset()\n",
    "        input_tokens = [initial_token_id]\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        comparisons = []\n",
    "        \n",
    "        state = 'expect_action'\n",
    "        done = False\n",
    "        success = False\n",
    "\n",
    "        while not done and len(input_tokens) < env.max_steps:\n",
    "            if verbose:\n",
    "                print(decode(input_tokens, inv_vocab))\n",
    "                print(env.get_list())\n",
    "                print(comparisons)\n",
    "            # Prepare input tensor\n",
    "            input_seq = torch.tensor(input_tokens, dtype=torch.long, device=device).unsqueeze(1)  # (seq_len, batch_size)\n",
    "            # Get model output\n",
    "            with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "                output = model(input_seq)  # (seq_len, batch_size, vocab_size)\n",
    "                # Get logits for the last token\n",
    "                logits = output[-1, 0, :]  # (vocab_size)\n",
    "\n",
    "                # Check for NaNs in logits\n",
    "                if torch.isnan(logits).any():\n",
    "                    print(f\"Episode {episode}, NaNs in logits before masking.\")\n",
    "                    break\n",
    "\n",
    "                # Get valid tokens based on state\n",
    "                def get_valid_tokens(state):\n",
    "                    action_tokens = [vocab['Comparison'], vocab['Swap']]\n",
    "                    index_tokens = [vocab[str(i)] for i in range(env.length)]\n",
    "                    if state == 'expect_action':\n",
    "                        return action_tokens\n",
    "                    elif state == 'expect_index1':\n",
    "                        return index_tokens[:-1]\n",
    "                    elif state == 'expect_index2':\n",
    "                        return [x for x in index_tokens if x > input_tokens[-1]]\n",
    "                    else:\n",
    "                        # Handle unexpected states by defaulting to expect_action\n",
    "                        return action_tokens\n",
    "\n",
    "                valid_token_ids = get_valid_tokens(state)\n",
    "\n",
    "                # Ensure valid_token_ids are within the vocab range\n",
    "                if any(idx >= vocab_size or idx < 0 for idx in valid_token_ids):\n",
    "                    print(f\"Episode {episode}, invalid indices in valid_token_ids: {valid_token_ids}\")\n",
    "                    break\n",
    "\n",
    "                # Mask invalid tokens\n",
    "                mask_value = -1e9  # Use a large negative value instead of -inf\n",
    "                mask = torch.full_like(logits, mask_value).to(device)\n",
    "                mask[valid_token_ids] = 0\n",
    "                masked_logits = logits + mask\n",
    "\n",
    "                # Sample action. Have some chance to randomly pick a valid action.\n",
    "                eps_threshold = EPS_END + (EPS_START - EPS_END) * np.exp(-1.0 * episode / EPS_DECAY)\n",
    "                if random.random() < eps_threshold:\n",
    "                    masked_logits = masked_logits / 4\n",
    "\n",
    "                # Check for NaNs in masked_logits\n",
    "                if torch.isnan(masked_logits).any():\n",
    "                    print(f\"Episode {episode}, NaNs in masked_logits after masking.\")\n",
    "                    break\n",
    "\n",
    "                # Compute probabilities\n",
    "                probs = F.softmax(masked_logits, dim=0)\n",
    "\n",
    "                # Check for NaNs in probs\n",
    "                if torch.isnan(probs).any():\n",
    "                    print(f\"Episode {episode}, NaNs in probs after softmax.\")\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    m = torch.distributions.Categorical(probs)\n",
    "                    action_token = m.sample()\n",
    "                    log_prob = m.log_prob(action_token)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Episode {episode}, error in sampling action: {e}\")\n",
    "                    break\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            input_tokens.append(action_token.item())\n",
    "\n",
    "            action = action_token.item()\n",
    "            reward = 0.0\n",
    "            if state == 'expect_action':\n",
    "                if action == vocab['Comparison']:\n",
    "                    state = 'expect_index1'\n",
    "                elif action == vocab['Swap']:\n",
    "                    if env.indices is None:\n",
    "                        reward = INVALID_ACTION_REWARD\n",
    "                        rewards.append(reward)\n",
    "                        done = True\n",
    "                        continue\n",
    "                    action_tokens = [vocab['Swap']]\n",
    "                    response_token, reward, done, current_list = env.step(action_tokens)\n",
    "                    if done and reward == SUCCESS_REWARD:\n",
    "                        success = True\n",
    "                        if episode % 1 == 0:\n",
    "                            print(decode(input_tokens, inv_vocab))\n",
    "                    if verbose:\n",
    "                        print(\"Reward:\", reward)\n",
    "                    state = 'expect_action'\n",
    "                else:\n",
    "                    reward = INVALID_ACTION_REWARD\n",
    "                    done = True\n",
    "            elif state == 'expect_index1':\n",
    "                index1_token = action_token\n",
    "                state = 'expect_index2'\n",
    "            elif state == 'expect_index2':\n",
    "                index2_token = action_token\n",
    "                action_tokens = [vocab['Comparison'], index1_token.item(), index2_token.item()]\n",
    "                comparisons.append((int(inv_vocab[index1_token.item()]), \n",
    "                                    int(inv_vocab[index2_token.item()])))\n",
    "                response_token, reward, done, current_list = env.step(action_tokens)\n",
    "                if done and reward == SUCCESS_REWARD:\n",
    "                    success = True\n",
    "                    if episode % 100 == 0:\n",
    "                        print(1, decode(input_tokens, inv_vocab))\n",
    "                else:\n",
    "                    reward += COMPARISON_ENTROPY_MULTIPLIER * compute_min_delta_entropy(comparisons)\n",
    "                if verbose:\n",
    "                    print(\"Reward:\", reward)\n",
    "                if response_token is not None:\n",
    "                    input_tokens.append(response_token)\n",
    "                state = 'expect_action'\n",
    "            else:\n",
    "                reward = INVALID_ACTION_REWARD\n",
    "                done = True\n",
    "\n",
    "            rewards.append(reward)\n",
    "        #\n",
    "        success_rewards = [0.0] * len(rewards)\n",
    "        if success: \n",
    "            num_successes += 1\n",
    "            success_rewards[-1] = SUCCESS_REWARD\n",
    "\n",
    "        # Save checkpoint\n",
    "        if episode > 0 and episode % EPISODES_SAVE == 0:\n",
    "            avg_reward = total_reward / episode_cnt\n",
    "            success_rate = num_successes / episode_cnt\n",
    "            avg_steps = total_steps / episode_cnt\n",
    "            episode_cnt = 0\n",
    "            total_reward = 0.0\n",
    "            num_successes = 0\n",
    "            total_steps = 0\n",
    "            save_checkpoint(model, optimizer, episode, OUTPUT_DIR, f\"ckpt_{episode}_{success_rate:.4f}_{avg_steps:.2f}.pth\")\n",
    "            #\n",
    "            # Reduce the lr\n",
    "            scheduler.step()\n",
    "            # Optionally, log the learning rate\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            print(f\"Learning rate = {current_lr:.6f}\")\n",
    "        #\n",
    "        assert len(log_probs) == len(rewards), \"log_probs and returns have different sizes!\"\n",
    "\n",
    "        if len(log_probs) == 0:\n",
    "            continue  # Skip if no actions were taken\n",
    "\n",
    "        # Compute returns and loss within autocast\n",
    "        with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "            # Compute returns\n",
    "            returns1 = compute_bellman_returns(rewards, SHORTTERM_GAMMA)\n",
    "            returns2 = compute_bellman_returns(success_rewards, LONGTERM_GAMMA)\n",
    "            returns = torch.tensor(np.array(returns1) + np.array(returns2)).to(device)\n",
    "\n",
    "            # Check for NaNs in returns\n",
    "            if torch.isnan(returns).any():\n",
    "                print(f\"Episode {episode}, NaNs in returns.\")\n",
    "                continue\n",
    "\n",
    "            # Compute loss\n",
    "            loss = 0\n",
    "            for log_prob, R in zip(log_probs, returns):\n",
    "                loss -= log_prob * R\n",
    "\n",
    "            # Check for NaNs in loss\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"Episode {episode}, NaN in loss.\")\n",
    "                continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        episode_cnt += 1\n",
    "        total_reward += sum(rewards)\n",
    "        total_steps += len(rewards)\n",
    "        t2 = time.time()\n",
    "        if episode % 1 == 0:\n",
    "            print(f\"Episode {episode}, loss:{loss.item():.4f}, {'succeed' if success else 'fail'}, steps:{len(rewards)}, total reward:{sum(rewards):.4f}, {t2-t1} sec\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train(verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4fc45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5975178",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0db249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5516d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_entropy(N, alpha=1):\n",
    "    K = 2**N\n",
    "    values = np.arange(K)\n",
    "    unnormalized_probs = np.exp(-alpha * values)\n",
    "    Z = unnormalized_probs.sum()\n",
    "    probs = unnormalized_probs / Z\n",
    "    return values, -np.log2(probs)\n",
    "\n",
    "# Parameters\n",
    "N = 4  # Number of bits\n",
    "alpha = 1  # Decay rate\n",
    "\n",
    "# Compute probabilities\n",
    "values, probs = compute_entropy(N, alpha)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(values, probs, width=0.8, alpha=0.7, edgecolor=\"black\")\n",
    "plt.title(f\"Exponential Decay Probabilities (N={N}, alpha={alpha})\")\n",
    "plt.xlabel(\"Integer Value\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61755053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_min_delta(comparisons):\n",
    "    # Initialize the result list to store minDelta values\n",
    "    min_delta = []\n",
    "\n",
    "    # Iterate through each pair in the comparisons list\n",
    "    for i, (xi, yi) in enumerate(comparisons):\n",
    "        if i == 0:\n",
    "            # For i = 0, use the first case directly\n",
    "            min_delta.append((xi, min(yi, yi - xi), 0))\n",
    "        else:\n",
    "            # For i > 0, compute all possible options and select the minimal one\n",
    "            options = []\n",
    "\n",
    "            # Simple Entropy\n",
    "            simple_entropy = (xi, min(yi, yi - xi), 0)\n",
    "            options.append(simple_entropy)\n",
    "\n",
    "            # First Delta Entropy\n",
    "            xi_prev, yi_prev = comparisons[i - 1]\n",
    "            first_delta_entropy = (xi - xi_prev, yi - yi_prev, 0)\n",
    "            options.append(first_delta_entropy)\n",
    "\n",
    "            # Second Delta Entropy (only valid for i > 1)\n",
    "            if i > 1:\n",
    "                xi_prev2, yi_prev2 = comparisons[i - 2]\n",
    "                second_delta_entropy = (\n",
    "                    (xi - xi_prev) - (xi_prev - xi_prev2),\n",
    "                    (yi - yi_prev) - (yi_prev - yi_prev2),\n",
    "                    0,\n",
    "                )\n",
    "                options.append(second_delta_entropy)\n",
    "\n",
    "            # Arbitrary Position Entropy (only valid for i > 1)\n",
    "            for j in range(i):\n",
    "                xj, yj = comparisons[j]\n",
    "                arbitrary_position_entropy = (\n",
    "                    xi - xj,\n",
    "                    yi - yj,\n",
    "                    min(j, i - j),\n",
    "                )\n",
    "                options.append(arbitrary_position_entropy)\n",
    "\n",
    "            # Find the option with the minimal sum\n",
    "            min_delta.append(min(options, key=lambda t: sum([abs(x) for x in t])))\n",
    "\n",
    "    return min_delta\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "comparisons = [(1, 5), (2, 7), (4, 10), (8, 15)]\n",
    "result = compute_min_delta(comparisons)\n",
    "\n",
    "# Output the results\n",
    "for i, delta in enumerate(result):\n",
    "    print(f\"minDelta for comparison {i}: {delta}\")\n",
    "\n",
    "def compute_min_delta_entropy(comparisons):\n",
    "    # Initialize the result list to store minDelta values\n",
    "    min_delta = None\n",
    "\n",
    "    # Iterate through each pair in the comparisons list\n",
    "    i = len(comparisons) - 1\n",
    "    xi, yi = comparisons[i]\n",
    "    if i == 0:\n",
    "        # For i = 0, use the first case directly\n",
    "        min_delta = (xi, min(yi, yi - xi), 0)\n",
    "    else:\n",
    "        # For i > 0, compute all possible options and select the minimal one\n",
    "        options = []\n",
    "\n",
    "        # Simple Entropy\n",
    "        simple_entropy = (xi, min(yi, yi - xi), 0)\n",
    "        options.append(simple_entropy)\n",
    "\n",
    "        # First Delta Entropy\n",
    "        xi_prev, yi_prev = comparisons[i - 1]\n",
    "        first_delta_entropy = (xi - xi_prev, yi - yi_prev, 0)\n",
    "        options.append(first_delta_entropy)\n",
    "\n",
    "        # Second Delta Entropy (only valid for i > 1)\n",
    "        if i > 1:\n",
    "            xi_prev2, yi_prev2 = comparisons[i - 2]\n",
    "            second_delta_entropy = (\n",
    "                (xi - xi_prev) - (xi_prev - xi_prev2),\n",
    "                (yi - yi_prev) - (yi_prev - yi_prev2),\n",
    "                0,\n",
    "            )\n",
    "            options.append(second_delta_entropy)\n",
    "\n",
    "        # Arbitrary Position Entropy (only valid for i > 1)\n",
    "        for j in range(i):\n",
    "            xj, yj = comparisons[j]\n",
    "            arbitrary_position_entropy = (\n",
    "                xi - xj,\n",
    "                yi - yj,\n",
    "                min(j, i - j),\n",
    "            )\n",
    "            options.append(arbitrary_position_entropy)\n",
    "\n",
    "        # Find the option with the minimal sum\n",
    "        min_delta = min(options, key=lambda t: sum([get_entropy_of_integer(x) for x in t]))\n",
    "\n",
    "    return sum([get_entropy_of_integer(x) for x in min_delta])\n",
    "\n",
    "compute_min_delta_entropy(comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d792c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_case_1():\n",
    "    # Test with a single comparison\n",
    "    comparisons = [(3, 8)]\n",
    "    expected_result = [(3, 5, 0)]\n",
    "    assert compute_min_delta(comparisons) == expected_result\n",
    "\n",
    "def test_case_2():\n",
    "    # Test with multiple comparisons\n",
    "    comparisons = [(1, 5), (2, 7), (4, 10), (8, 15)]\n",
    "    expected_result = [\n",
    "        (1, 4, 0),  # Simple Entropy for the first pair\n",
    "        (1, 2, 0),  # Minimal sum using First Delta Entropy\n",
    "        (1, 1, 0),  # Minimal sum using First Delta Entropy\n",
    "        (2, 2, 0),  # Minimal sum using First Delta Entropy\n",
    "    ]\n",
    "    assert compute_min_delta(comparisons) == expected_result\n",
    "\n",
    "def test_case_3():\n",
    "    # Test with comparisons where the minimal sum comes from Arbitrary Position Entropy\n",
    "    comparisons = [(1, 10), (3, 12), (5, 14), (1, 11)]\n",
    "    expected_result = [\n",
    "        (1, 9, 0),  # Simple Entropy for the first pair\n",
    "        (2, 2, 0),  # Minimal sum using First Delta Entropy\n",
    "        (0, 0, 0),  # Minimal sum using First Delta Entropy\n",
    "        (0, 1, 0),  # Minimal sum using Arbitrary Position Entropy\n",
    "    ]\n",
    "    assert compute_min_delta(comparisons) == expected_result\n",
    "\n",
    "test_case_1()\n",
    "test_case_2()\n",
    "test_case_3()\n",
    "print(\"All cases succeeded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce1ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rewards = [-1, 1, -1, 1, -1, 1]\n",
    "success_rewards = [0.0] * len(rewards)\n",
    "success_rewards[-1] = 0.5\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "r1 = compute_bellman_returns(rewards, 0.7)\n",
    "r2 = compute_bellman_returns(success_rewards, 0.99)\n",
    "print(r1)\n",
    "print(r2)\n",
    "np.array(r1) + np.array(r2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axiom",
   "language": "python",
   "name": "axiom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
