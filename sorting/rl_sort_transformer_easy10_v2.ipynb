{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ba0d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "\n",
    "MIN_LIST_LEN = 10\n",
    "MAX_LIST_LEN = 10\n",
    "MAX_STEPS = 330\n",
    "\n",
    "SUCCESS_REWARD = 2.0\n",
    "STEP_REWARD = -0.2\n",
    "COMPARISON_ENTROPY_MULTIPLIER = -0.05\n",
    "SWAP_REWARD = 1.0\n",
    "INVALID_ACTION_REWARD = -10.0\n",
    "\n",
    "EPS_START = 0.5\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "GAMMA = 0.9\n",
    "NUM_EPISODES = 100000\n",
    "EPISODES_SAVE = 1000\n",
    "OUTPUT_DIR = 'datasets/rl_sort_transformer_easy/list10_transformer3_128_gamma08_step330_v2'\n",
    "\n",
    "# Define the vocabulary\n",
    "vocab = {\n",
    "    'Comparison': 0,\n",
    "    'Swap': 1,\n",
    "    'less': 2,\n",
    "    'equal': 3,\n",
    "    'more': 4,\n",
    "    '0': 5,\n",
    "    '1': 6,\n",
    "    '2': 7,\n",
    "    '3': 8,\n",
    "    '4': 9,\n",
    "    '5': 10,\n",
    "    '6': 11,\n",
    "    '7': 12,\n",
    "    '8': 13,\n",
    "    '9': 14,\n",
    "    '10': 15,\n",
    "    '11': 16,\n",
    "    '12': 17,\n",
    "    '13': 18,\n",
    "    '14': 19,\n",
    "    '15': 20,\n",
    "    'len1': 21,\n",
    "    'len2': 22,\n",
    "    'len3': 23,\n",
    "    'len4': 24,\n",
    "    'len5': 25,\n",
    "    'len6': 26,\n",
    "    'len7': 27,\n",
    "    'len8': 28,\n",
    "    'len9': 29,\n",
    "    'len10': 30,\n",
    "    'len11': 31,\n",
    "    'len12': 32,\n",
    "    'len13': 33,\n",
    "    'len14': 34,\n",
    "    'len15': 35,\n",
    "    'len16': 36,\n",
    "}\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "def compute_entropy(N, alpha=1):\n",
    "    K = 2**N\n",
    "    values = np.arange(K)\n",
    "    unnormalized_probs = np.exp(-alpha * values)\n",
    "    Z = unnormalized_probs.sum()\n",
    "    probs = unnormalized_probs / Z\n",
    "    return values, -np.log2(probs)\n",
    "\n",
    "_, int_entropy = compute_entropy(4)\n",
    "\n",
    "def get_entropy_of_integer(x):\n",
    "    x = min(15, abs(x))\n",
    "    return int_entropy[x]\n",
    "\n",
    "def compute_min_delta_entropy(comparisons):\n",
    "    # Initialize the result list to store minDelta values\n",
    "    min_delta = None\n",
    "\n",
    "    # Iterate through each pair in the comparisons list\n",
    "    i = len(comparisons) - 1\n",
    "    xi, yi = comparisons[i]\n",
    "    if i == 0:\n",
    "        # For i = 0, use the first case directly\n",
    "        min_delta = (xi, min(yi, yi - xi), 0)\n",
    "    else:\n",
    "        # For i > 0, compute all possible options and select the minimal one\n",
    "        options = []\n",
    "\n",
    "        # Simple Entropy\n",
    "        simple_entropy = (xi, min(yi, yi - xi), 0)\n",
    "        options.append(simple_entropy)\n",
    "\n",
    "        # First Delta Entropy\n",
    "        xi_prev, yi_prev = comparisons[i - 1]\n",
    "        first_delta_entropy = (xi - xi_prev, yi - yi_prev, 0)\n",
    "        options.append(first_delta_entropy)\n",
    "\n",
    "        # Second Delta Entropy (only valid for i > 1)\n",
    "        if i > 1:\n",
    "            xi_prev2, yi_prev2 = comparisons[i - 2]\n",
    "            second_delta_entropy = (\n",
    "                (xi - xi_prev) - (xi_prev - xi_prev2),\n",
    "                (yi - yi_prev) - (yi_prev - yi_prev2),\n",
    "                0,\n",
    "            )\n",
    "            options.append(second_delta_entropy)\n",
    "\n",
    "        # Arbitrary Position Entropy (only valid for i > 1)\n",
    "        for j in range(i):\n",
    "            xj, yj = comparisons[j]\n",
    "            arbitrary_position_entropy = (\n",
    "                xi - xj,\n",
    "                yi - yj,\n",
    "                min(j, i - j),\n",
    "            )\n",
    "            options.append(arbitrary_position_entropy)\n",
    "\n",
    "        # Find the option with the minimal sum\n",
    "        min_delta = min(options, key=lambda t: sum([get_entropy_of_integer(x) for x in t]))\n",
    "\n",
    "    return sum([get_entropy_of_integer(x) for x in min_delta])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the environment\n",
    "class SortingEnv:\n",
    "    def __init__(self):\n",
    "        self.max_steps = MAX_STEPS\n",
    "\n",
    "    def reset(self):\n",
    "        self.length = random.randint(MIN_LIST_LEN, MAX_LIST_LEN)\n",
    "        self.list = [random.randint(1, 100) for _ in range(self.length)]\n",
    "        while self.list == sorted(self.list):\n",
    "            self.list = [random.randint(1, 100) for _ in range(self.length)]\n",
    "        self.indices = None\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        initial_token = 'len{}'.format(self.length)\n",
    "        return vocab[initial_token], self.list.copy()\n",
    "    \n",
    "    def get_list(self):\n",
    "        return self.list\n",
    "    \n",
    "    def get_list_len(self):\n",
    "        return len(self.list)\n",
    "\n",
    "    def step(self, action_tokens):\n",
    "        action = action_tokens[0]\n",
    "        reward = -0.01  # default penalty\n",
    "        response_token = None\n",
    "\n",
    "        if action == vocab['Comparison']:\n",
    "            if len(action_tokens) != 3:\n",
    "                reward = INVALID_ACTION_REWARD\n",
    "                self.done = True\n",
    "                return response_token, reward, self.done, self.list.copy()\n",
    "            index1 = action_tokens[1] - vocab['0']\n",
    "            index2 = action_tokens[2] - vocab['0']\n",
    "            if index1 >= self.length or index2 >= self.length or index1 < 0 or index2 < 0:\n",
    "                reward = INVALID_ACTION_REWARD\n",
    "                self.done = True\n",
    "                return response_token, reward, self.done, self.list.copy()\n",
    "            self.indices = (index1, index2)\n",
    "            if self.list[index1] < self.list[index2]:\n",
    "                response_token = vocab['less']\n",
    "                reward = STEP_REWARD\n",
    "            elif self.list[index1] == self.list[index2]:\n",
    "                response_token = vocab['equal']\n",
    "                reward = STEP_REWARD * 2\n",
    "            else:\n",
    "                response_token = vocab['more']\n",
    "                reward = STEP_REWARD\n",
    "        elif action == vocab['Swap']:\n",
    "            if self.indices is None:\n",
    "                reward = INVALID_ACTION_REWARD\n",
    "                self.done = True\n",
    "                return response_token, reward, self.done, self.list.copy()\n",
    "            index1, index2 = self.indices\n",
    "            prev_list = self.list.copy()\n",
    "            self.list[index1], self.list[index2] = self.list[index2], self.list[index1]\n",
    "            if self.list == sorted(self.list):\n",
    "                reward = SUCCESS_REWARD\n",
    "                self.done = True\n",
    "            #elif prev_list[index1] > prev_list[index2] and self.list[index1] <= self.list[index2]:\n",
    "            #    reward = 0.1\n",
    "            elif (index1 < index2 and prev_list[index1] > prev_list[index2] and self.list[index1] <= self.list[index2]) or \\\n",
    "                (index1 > index2 and prev_list[index1] < prev_list[index2] and self.list[index1] >= self.list[index2]):\n",
    "                reward = SWAP_REWARD\n",
    "            elif (index1 < index2 and prev_list[index1] < prev_list[index2] and self.list[index1] >= self.list[index2]) or \\\n",
    "                (index1 > index2 and prev_list[index1] > prev_list[index2] and self.list[index1] <= self.list[index2]):\n",
    "                reward = -SWAP_REWARD\n",
    "            else:\n",
    "                reward = STEP_REWARD\n",
    "            self.indices = None\n",
    "        else:\n",
    "            reward = INVALID_ACTION_REWARD\n",
    "            self.done = True\n",
    "\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            self.done = True\n",
    "        return response_token, reward, self.done, self.list.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0cf9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding for Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "        pe = pe.unsqueeze(1)  # (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=8, num_layers=3):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.embedding.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "def decode(input_tokens, inv_vocab):\n",
    "    return ' '.join([inv_vocab[x] for x in input_tokens])\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, episode, folder, filename):\n",
    "    \"\"\"\n",
    "    Save the model and optimizer state to the designated filepath.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to save.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer whose state to save.\n",
    "        episode (int): The current episode number.\n",
    "        filepath (str): The path where to save the checkpoint.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    # Save the checkpoint\n",
    "    torch.save({\n",
    "        'episode': episode,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, filepath)\n",
    "    print(f\"Checkpoint saved at episode {episode} to {filepath}\")\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer):\n",
    "    \"\"\"\n",
    "    Load the model and optimizer state from the designated filepath.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path from where to load the checkpoint.\n",
    "        model (nn.Module): The model into which to load the state_dict.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer into which to load the state.\n",
    "\n",
    "    Returns:\n",
    "        int: The episode number to resume from.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        episode = checkpoint['episode']\n",
    "        print(f\"Checkpoint loaded from {filepath}, resuming from episode {episode}\")\n",
    "        return episode\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {filepath}, starting from scratch.\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a13cca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from datasets/rl_sort_transformer_easy/list10_transformer3_128_gamma08_step330_v2/ckpt_52000_0.6910_169.16.pth, resuming from episode 52000\n",
      "(1, 2) 4.870574667602986\n",
      "(2, 3) 4.870574667602986\n",
      "(3, 4) 1.985184585825059\n",
      "(4, 5) 1.985184585825059\n",
      "(5, 6) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(3, 4) 1.985184585825059\n",
      "(5, 6) 7.7559647493809125\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(8, 9) 1.985184585825059\n",
      "(0, 1) 3.427879626714023\n",
      "(4, 5) 6.31326970849195\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(5, 6) 1.985184585825059\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(4, 5) 6.31326970849195\n",
      "(6, 7) 1.985184585825059\n",
      "(5, 6) 4.870574667602986\n",
      "(4, 5) 1.985184585825059\n",
      "(3, 4) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(1, 2) 1.985184585825059\n",
      "(0, 1) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(4, 5) 6.31326970849195\n",
      "(6, 7) 4.870574667602986\n",
      "(6, 7) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(6, 7) 4.870574667602986\n",
      "(6, 7) 1.985184585825059\n",
      "(5, 6) 4.870574667602986\n",
      "(4, 5) 1.985184585825059\n",
      "(3, 4) 1.985184585825059\n",
      "(2, 3) 1.985184585825059\n",
      "(2, 3) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(4, 5) 4.870574667602986\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(0, 1) 3.427879626714023\n",
      "(3, 4) 4.870574667602986\n",
      "(0, 1) 3.427879626714023\n",
      "(3, 4) 4.870574667602986\n",
      "(3, 4) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "Episode 0, loss:-9.7404, fail, steps:254, total reward:2.9488, 1.5262389183044434 sec\n",
      "(1, 2) 4.870574667602986\n",
      "(2, 3) 4.870574667602986\n",
      "(3, 4) 1.985184585825059\n",
      "(4, 5) 1.985184585825059\n",
      "(5, 6) 1.985184585825059\n",
      "(7, 8) 4.870574667602986\n",
      "(0, 1) 3.427879626714023\n",
      "(3, 4) 4.870574667602986\n",
      "(5, 6) 4.870574667602986\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(8, 9) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(4, 5) 6.31326970849195\n",
      "(3, 4) 4.870574667602986\n",
      "(5, 6) 7.7559647493809125\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(7, 8) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(2, 3) 1.985184585825059\n",
      "(4, 5) 6.31326970849195\n",
      "(6, 7) 1.985184585825059\n",
      "(6, 7) 1.985184585825059\n",
      "(5, 6) 4.870574667602986\n",
      "(4, 5) 1.985184585825059\n",
      "(3, 4) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(1, 2) 1.985184585825059\n",
      "(0, 1) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(4, 5) 6.31326970849195\n",
      "len10 Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 less Comparison 4 5 less Comparison 5 6 less Comparison 7 8 more Swap Comparison 0 1 more Swap Comparison 3 4 less Comparison 5 6 less Comparison 6 7 more Swap Comparison 7 8 more Swap Comparison 8 9 more Swap Comparison 1 2 more Swap Comparison 4 5 less Comparison 3 4 less Comparison 5 6 less Comparison 6 7 less Comparison 7 8 more Swap Comparison 7 8 less Comparison 2 3 more Swap Comparison 2 3 less Comparison 4 5 less Comparison 6 7 more Swap Comparison 6 7 less Comparison 5 6 more Swap Comparison 4 5 less Comparison 3 4 more Swap Comparison 1 2 less Comparison 1 2 less Comparison 2 3 less Comparison 1 2 less Comparison 0 1 less Comparison 1 2 less Comparison 4 5 more Swap\n",
      "Episode 1, loss:0.6129, succeed, steps:116, total reward:2.3726, 0.48689794540405273 sec\n",
      "(1, 2) 4.870574667602986\n",
      "(2, 3) 4.870574667602986\n",
      "(3, 4) 1.985184585825059\n",
      "(4, 5) 1.985184585825059\n",
      "(0, 1) 3.427879626714023\n",
      "(1, 2) 1.985184585825059\n",
      "(2, 3) 1.985184585825059\n",
      "(3, 4) 1.985184585825059\n",
      "(5, 6) 4.870574667602986\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(8, 9) 1.985184585825059\n",
      "(4, 5) 6.31326970849195\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(5, 6) 1.985184585825059\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(4, 5) 6.31326970849195\n",
      "(4, 5) 1.985184585825059\n",
      "(6, 7) 7.7559647493809125\n",
      "(5, 6) 4.870574667602986\n",
      "(4, 5) 1.985184585825059\n",
      "(3, 4) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(1, 2) 1.985184585825059\n",
      "(0, 1) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(4, 5) 6.31326970849195\n",
      "(6, 7) 4.870574667602986\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(6, 7) 6.31326970849195\n",
      "(6, 7) 1.985184585825059\n",
      "(5, 6) 4.870574667602986\n",
      "(4, 5) 1.985184585825059\n",
      "(3, 4) 1.985184585825059\n",
      "(3, 4) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(1, 2) 1.985184585825059\n",
      "len10 Comparison 1 2 less Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 0 1 more Swap Comparison 1 2 less Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 5 6 more Swap Comparison 6 7 more Swap Comparison 7 8 more Swap Comparison 8 9 more Swap Comparison 4 5 more Swap Comparison 1 2 more Swap Comparison 3 4 less Comparison 5 6 more Swap Comparison 6 7 more Swap Comparison 7 8 more Swap Comparison 2 3 more Swap Comparison 4 5 less Comparison 4 5 less Comparison 6 7 more Swap Comparison 5 6 more Swap Comparison 4 5 more Swap Comparison 3 4 more Swap Comparison 1 2 less Comparison 1 2 less Comparison 2 3 more Swap Comparison 1 2 more Swap Comparison 0 1 more Swap Comparison 1 2 less Comparison 4 5 more Swap Comparison 6 7 more Swap Comparison 1 2 less Comparison 1 2 less Comparison 6 7 less Comparison 6 7 less Comparison 5 6 more Swap Comparison 4 5 more Swap Comparison 3 4 more Swap Comparison 3 4 less Comparison 2 3 more Swap Comparison 1 2 more Swap\n",
      "Episode 2, loss:0.2166, succeed, steps:159, total reward:15.3907, 0.7569797039031982 sec\n",
      "(1, 2) 4.870574667602986\n",
      "(2, 3) 4.870574667602986\n",
      "(3, 4) 1.985184585825059\n",
      "(4, 5) 1.985184585825059\n",
      "(0, 1) 3.427879626714023\n",
      "(1, 2) 1.985184585825059\n",
      "(2, 3) 1.985184585825059\n",
      "(3, 4) 1.985184585825059\n",
      "(5, 6) 4.870574667602986\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(8, 9) 1.985184585825059\n",
      "(4, 5) 6.31326970849195\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(5, 6) 1.985184585825059\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "len10 Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 0 1 less Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 less Comparison 5 6 more Swap Comparison 6 7 more Swap Comparison 7 8 more Swap Comparison 8 9 more Swap Comparison 4 5 more Swap Comparison 1 2 less Comparison 3 4 more Swap Comparison 5 6 more Swap Comparison 6 7 less Comparison 7 8 less Comparison 2 3 more Swap\n",
      "Episode 3, loss:0.0350, succeed, steps:71, total reward:8.0878, 0.3078172206878662 sec\n",
      "(1, 2) 4.870574667602986\n",
      "(2, 3) 4.870574667602986\n",
      "(3, 4) 1.985184585825059\n",
      "(4, 5) 1.985184585825059\n",
      "(5, 6) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(3, 4) 1.985184585825059\n",
      "(5, 6) 4.870574667602986\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(8, 9) 1.985184585825059\n",
      "(0, 1) 3.427879626714023\n",
      "(4, 5) 6.31326970849195\n",
      "(3, 4) 4.870574667602986\n",
      "(5, 6) 7.7559647493809125\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(4, 5) 6.31326970849195\n",
      "(6, 7) 1.985184585825059\n",
      "(6, 7) 1.985184585825059\n",
      "(5, 6) 4.870574667602986\n",
      "(4, 5) 1.985184585825059\n",
      "(3, 4) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(0, 1) 3.427879626714023\n",
      "(4, 5) 6.31326970849195\n",
      "(6, 7) 7.7559647493809125\n",
      "(6, 7) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "len10 Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 less Comparison 4 5 more Swap Comparison 5 6 more Swap Comparison 1 2 more Swap Comparison 2 3 less Comparison 3 4 more Swap Comparison 5 6 less Comparison 6 7 more Swap Comparison 7 8 more Swap Comparison 8 9 more Swap Comparison 0 1 more Swap Comparison 4 5 more Swap Comparison 3 4 more Swap Comparison 5 6 more Swap Comparison 6 7 less Comparison 7 8 more Swap Comparison 2 3 more Swap Comparison 4 5 more Swap Comparison 6 7 more Swap Comparison 6 7 less Comparison 5 6 less Comparison 4 5 less Comparison 3 4 more Swap Comparison 1 2 more Swap Comparison 1 2 less Comparison 2 3 more Swap Comparison 1 2 more Swap Comparison 1 2 less Comparison 0 1 more Swap Comparison 4 5 less Comparison 6 7 less Comparison 6 7 less Comparison 1 2 more Swap\n",
      "Episode 4, loss:0.1236, succeed, steps:128, total reward:10.9291, 0.5231015682220459 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2) 4.870574667602986\n",
      "(2, 3) 4.870574667602986\n",
      "(3, 4) 1.985184585825059\n",
      "(4, 5) 1.985184585825059\n",
      "(5, 6) 1.985184585825059\n",
      "(0, 1) 3.427879626714023\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(5, 6) 1.985184585825059\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(8, 9) 1.985184585825059\n",
      "(8, 9) 1.985184585825059\n",
      "(4, 5) 6.31326970849195\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(5, 6) 1.985184585825059\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(2, 3) 1.985184585825059\n",
      "(4, 5) 6.31326970849195\n",
      "(6, 7) 1.985184585825059\n",
      "(6, 7) 1.985184585825059\n",
      "(5, 6) 4.870574667602986\n",
      "(4, 5) 1.985184585825059\n",
      "(3, 4) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(0, 1) 3.427879626714023\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(4, 5) 6.31326970849195\n",
      "(6, 7) 4.870574667602986\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(6, 7) 4.870574667602986\n",
      "(6, 7) 1.985184585825059\n",
      "(5, 6) 4.870574667602986\n",
      "(4, 5) 1.985184585825059\n",
      "(3, 4) 1.985184585825059\n",
      "(2, 3) 1.985184585825059\n",
      "(2, 3) 1.985184585825059\n",
      "(4, 5) 6.31326970849195\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(3, 4) 1.985184585825059\n",
      "(4, 5) 4.870574667602986\n",
      "(0, 1) 3.427879626714023\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(3, 4) 1.985184585825059\n",
      "(0, 1) 3.427879626714023\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 3) 3.427879626714023\n",
      "(3, 4) 4.870574667602986\n",
      "(3, 4) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "Episode 5, loss:-17.2893, fail, steps:252, total reward:-10.1218, 1.1036226749420166 sec\n",
      "(1, 2) 4.870574667602986\n",
      "(2, 3) 4.870574667602986\n",
      "(3, 4) 1.985184585825059\n",
      "(4, 5) 1.985184585825059\n",
      "(0, 1) 3.427879626714023\n",
      "(1, 2) 1.985184585825059\n",
      "(2, 3) 1.985184585825059\n",
      "(3, 4) 1.985184585825059\n",
      "(5, 6) 4.870574667602986\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(8, 9) 1.985184585825059\n",
      "(4, 5) 6.31326970849195\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(5, 6) 1.985184585825059\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(4, 5) 6.31326970849195\n",
      "(4, 5) 1.985184585825059\n",
      "(6, 7) 7.7559647493809125\n",
      "(5, 6) 4.870574667602986\n",
      "(4, 5) 1.985184585825059\n",
      "(3, 4) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(1, 2) 1.985184585825059\n",
      "(0, 1) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(4, 5) 6.31326970849195\n",
      "(6, 7) 4.870574667602986\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(6, 7) 4.870574667602986\n",
      "(5, 6) 4.870574667602986\n",
      "(4, 5) 1.985184585825059\n",
      "(3, 4) 1.985184585825059\n",
      "(3, 4) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(4, 5) 6.31326970849195\n",
      "(1, 2) 1.985184585825059\n",
      "len10 Comparison 1 2 more Swap Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 4 5 more Swap Comparison 0 1 less Comparison 1 2 less Comparison 2 3 more Swap Comparison 3 4 more Swap Comparison 5 6 more Swap Comparison 6 7 more Swap Comparison 7 8 more Swap Comparison 8 9 more Swap Comparison 4 5 more Swap Comparison 1 2 less Comparison 3 4 less Comparison 5 6 more Swap Comparison 6 7 more Swap Comparison 7 8 more Swap Comparison 2 3 more Swap Comparison 4 5 more Swap Comparison 4 5 less Comparison 6 7 more Swap Comparison 5 6 more Swap Comparison 4 5 less Comparison 3 4 more Swap Comparison 1 2 more Swap Comparison 1 2 less Comparison 2 3 more Swap Comparison 1 2 more Swap Comparison 0 1 more Swap Comparison 1 2 less Comparison 4 5 more Swap Comparison 6 7 less Comparison 1 2 less Comparison 1 2 less Comparison 3 4 more Swap Comparison 6 7 less Comparison 5 6 more Swap Comparison 4 5 more Swap Comparison 3 4 less Comparison 3 4 less Comparison 2 3 more Swap Comparison 4 5 less Comparison 1 2 more Swap\n",
      "Episode 6, loss:0.3044, succeed, steps:161, total reward:13.8029, 0.7352688312530518 sec\n",
      "(1, 2) 4.870574667602986\n",
      "(2, 3) 4.870574667602986\n",
      "(3, 4) 1.985184585825059\n",
      "(4, 5) 1.985184585825059\n",
      "(5, 6) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(5, 6) 1.985184585825059\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(8, 9) 1.985184585825059\n",
      "(8, 9) 1.985184585825059\n",
      "(0, 1) 3.427879626714023\n",
      "(4, 5) 6.31326970849195\n",
      "(3, 4) 4.870574667602986\n",
      "(5, 6) 7.7559647493809125\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(7, 8) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(4, 5) 6.31326970849195\n",
      "(4, 5) 1.985184585825059\n",
      "(6, 7) 7.7559647493809125\n",
      "(5, 6) 4.870574667602986\n",
      "(4, 5) 1.985184585825059\n",
      "(3, 4) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(1, 2) 1.985184585825059\n",
      "(0, 1) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(4, 5) 6.31326970849195\n",
      "(6, 7) 4.870574667602986\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(6, 7) 4.870574667602986\n",
      "(5, 6) 4.870574667602986\n",
      "(4, 5) 1.985184585825059\n",
      "(4, 5) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(1, 2) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(2, 3) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(4, 5) 4.870574667602986\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(3, 4) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 3) 3.427879626714023\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "(1, 2) 1.985184585825059\n",
      "Episode 7, loss:-21.4899, fail, steps:252, total reward:-11.8268, 1.162355661392212 sec\n",
      "(1, 2) 4.870574667602986\n",
      "(2, 3) 4.870574667602986\n",
      "(3, 4) 1.985184585825059\n",
      "(4, 5) 1.985184585825059\n",
      "(0, 1) 3.427879626714023\n",
      "(1, 2) 1.985184585825059\n",
      "(2, 3) 1.985184585825059\n",
      "(3, 4) 1.985184585825059\n",
      "(5, 6) 4.870574667602986\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(8, 9) 1.985184585825059\n",
      "(4, 5) 6.31326970849195\n",
      "(1, 2) 1.985184585825059\n",
      "(3, 4) 4.870574667602986\n",
      "(5, 6) 1.985184585825059\n",
      "(6, 7) 4.870574667602986\n",
      "(7, 8) 1.985184585825059\n",
      "(2, 3) 3.427879626714023\n",
      "(2, 3) 1.985184585825059\n",
      "(4, 5) 6.31326970849195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "def train(verbose=False):\n",
    "    # Removed torch.autograd.set_detect_anomaly(True)\n",
    "    vocab_size = len(vocab)\n",
    "    model = TransformerModel(vocab_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Reduced learning rate\n",
    "    # Optionally, load a checkpoint\n",
    "    checkpoint_path = os.path.join(OUTPUT_DIR, \"ckpt_52000_0.6910_169.16.pth\")\n",
    "    load_checkpoint(checkpoint_path, model, optimizer)\n",
    "\n",
    "    episode_cnt = 0\n",
    "    total_reward = 0.0\n",
    "    num_successes = 0\n",
    "    total_steps = 0\n",
    "    \n",
    "    for episode in range(NUM_EPISODES):\n",
    "        t1 = time.time()\n",
    "        model.train()  # Set model to training mode\n",
    "        env = SortingEnv()\n",
    "        initial_token_id, current_list = env.reset()\n",
    "        input_tokens = [initial_token_id]\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        comparisons = []\n",
    "        \n",
    "        state = 'expect_action'\n",
    "        done = False\n",
    "        success = False\n",
    "\n",
    "        while not done and len(input_tokens) < env.max_steps:\n",
    "            if verbose:\n",
    "                print(decode(input_tokens, inv_vocab))\n",
    "                print(env.get_list())\n",
    "                print(comparisons)\n",
    "            # Prepare input tensor\n",
    "            input_seq = torch.tensor(input_tokens, dtype=torch.long, device=device).unsqueeze(1)  # (seq_len, batch_size)\n",
    "            # Get model output\n",
    "            with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "                output = model(input_seq)  # (seq_len, batch_size, vocab_size)\n",
    "                # Get logits for the last token\n",
    "                logits = output[-1, 0, :]  # (vocab_size)\n",
    "\n",
    "                # Check for NaNs in logits\n",
    "                if torch.isnan(logits).any():\n",
    "                    print(f\"Episode {episode}, NaNs in logits before masking.\")\n",
    "                    break\n",
    "\n",
    "                # Get valid tokens based on state\n",
    "                def get_valid_tokens(state):\n",
    "                    action_tokens = [vocab['Comparison'], vocab['Swap']]\n",
    "                    index_tokens = [vocab[str(i)] for i in range(env.length)]\n",
    "                    if state == 'expect_action':\n",
    "                        return action_tokens\n",
    "                    elif state == 'expect_index1':\n",
    "                        return index_tokens[:-1]\n",
    "                    elif state == 'expect_index2':\n",
    "                        return [x for x in index_tokens if x > input_tokens[-1]]\n",
    "                    else:\n",
    "                        # Handle unexpected states by defaulting to expect_action\n",
    "                        return action_tokens\n",
    "\n",
    "                valid_token_ids = get_valid_tokens(state)\n",
    "\n",
    "                # Ensure valid_token_ids are within the vocab range\n",
    "                if any(idx >= vocab_size or idx < 0 for idx in valid_token_ids):\n",
    "                    print(f\"Episode {episode}, invalid indices in valid_token_ids: {valid_token_ids}\")\n",
    "                    break\n",
    "\n",
    "                # Mask invalid tokens\n",
    "                mask_value = -1e9  # Use a large negative value instead of -inf\n",
    "                mask = torch.full_like(logits, mask_value).to(device)\n",
    "                mask[valid_token_ids] = 0\n",
    "                masked_logits = logits + mask\n",
    "\n",
    "                # Sample action. Have some chance to randomly pick a valid action.\n",
    "                eps_threshold = EPS_END + (EPS_START - EPS_END) * np.exp(-1.0 * episode / EPS_DECAY)\n",
    "                if random.random() < eps_threshold:\n",
    "                    masked_logits = masked_logits / 4\n",
    "\n",
    "                # Check for NaNs in masked_logits\n",
    "                if torch.isnan(masked_logits).any():\n",
    "                    print(f\"Episode {episode}, NaNs in masked_logits after masking.\")\n",
    "                    break\n",
    "\n",
    "                # Compute probabilities\n",
    "                probs = F.softmax(masked_logits, dim=0)\n",
    "\n",
    "                # Check for NaNs in probs\n",
    "                if torch.isnan(probs).any():\n",
    "                    print(f\"Episode {episode}, NaNs in probs after softmax.\")\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    m = torch.distributions.Categorical(probs)\n",
    "                    action_token = m.sample()\n",
    "                    log_prob = m.log_prob(action_token)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Episode {episode}, error in sampling action: {e}\")\n",
    "                    break\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            input_tokens.append(action_token.item())\n",
    "\n",
    "            action = action_token.item()\n",
    "            reward = 0.0\n",
    "            if state == 'expect_action':\n",
    "                if action == vocab['Comparison']:\n",
    "                    state = 'expect_index1'\n",
    "                elif action == vocab['Swap']:\n",
    "                    if env.indices is None:\n",
    "                        reward = INVALID_ACTION_REWARD\n",
    "                        rewards.append(reward)\n",
    "                        done = True\n",
    "                        continue\n",
    "                    action_tokens = [vocab['Swap']]\n",
    "                    response_token, reward, done, current_list = env.step(action_tokens)\n",
    "                    if done and reward == SUCCESS_REWARD:\n",
    "                        success = True\n",
    "                        print(decode(input_tokens, inv_vocab))\n",
    "                    if verbose:\n",
    "                        print(\"Reward:\", reward)\n",
    "                    state = 'expect_action'\n",
    "                else:\n",
    "                    reward = INVALID_ACTION_REWARD\n",
    "                    done = True\n",
    "            elif state == 'expect_index1':\n",
    "                index1_token = action_token\n",
    "                state = 'expect_index2'\n",
    "            elif state == 'expect_index2':\n",
    "                index2_token = action_token\n",
    "                action_tokens = [vocab['Comparison'], index1_token.item(), index2_token.item()]\n",
    "                comparisons.append((int(inv_vocab[index1_token.item()]), \n",
    "                                    int(inv_vocab[index2_token.item()])))\n",
    "                response_token, reward, done, current_list = env.step(action_tokens)\n",
    "                if done and reward == SUCCESS_REWARD:\n",
    "                    success = True\n",
    "                    print(decode(input_tokens, inv_vocab))\n",
    "                else:\n",
    "                    reward += COMPARISON_ENTROPY_MULTIPLIER * compute_min_delta_entropy(comparisons)\n",
    "                    print(comparisons[-1], compute_min_delta_entropy(comparisons))\n",
    "                if verbose:\n",
    "                    print(\"Reward:\", reward)\n",
    "                if response_token is not None:\n",
    "                    input_tokens.append(response_token)\n",
    "                state = 'expect_action'\n",
    "            else:\n",
    "                reward = INVALID_ACTION_REWARD\n",
    "                done = True\n",
    "\n",
    "            rewards.append(reward)\n",
    "        #\n",
    "        if success: \n",
    "            num_successes += 1\n",
    "        # Save checkpoint\n",
    "        if episode > 0 and episode % EPISODES_SAVE == 0:\n",
    "            avg_reward = total_reward / episode_cnt\n",
    "            success_rate = num_successes / episode_cnt\n",
    "            avg_steps = total_steps / episode_cnt\n",
    "            episode_cnt = 0\n",
    "            total_reward = 0.0\n",
    "            num_successes = 0\n",
    "            total_steps = 0\n",
    "            save_checkpoint(model, optimizer, episode, OUTPUT_DIR, f\"ckpt_{episode}_{success_rate:.4f}_{avg_steps:.2f}.pth\")\n",
    "        #\n",
    "        assert len(log_probs) == len(rewards), \"log_probs and returns have different sizes!\"\n",
    "\n",
    "        if len(log_probs) == 0:\n",
    "            continue  # Skip if no actions were taken\n",
    "\n",
    "        # Compute returns and loss within autocast\n",
    "        with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "            # Compute returns\n",
    "            returns = []\n",
    "            R = 0\n",
    "            gamma = GAMMA\n",
    "            for r in rewards[::-1]:\n",
    "                R = r + gamma * R\n",
    "                returns.insert(0, R)\n",
    "            returns = torch.tensor(returns).to(device)\n",
    "\n",
    "            # Check for NaNs in returns\n",
    "            if torch.isnan(returns).any():\n",
    "                print(f\"Episode {episode}, NaNs in returns.\")\n",
    "                continue\n",
    "\n",
    "            # Compute loss\n",
    "            loss = 0\n",
    "            for log_prob, R in zip(log_probs, returns):\n",
    "                loss -= log_prob * R\n",
    "\n",
    "            # Check for NaNs in loss\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"Episode {episode}, NaN in loss.\")\n",
    "                continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        episode_cnt += 1\n",
    "        total_reward += sum(rewards)\n",
    "        total_steps += len(rewards)\n",
    "        t2 = time.time()\n",
    "        if episode % 1 == 0:\n",
    "            print(f\"Episode {episode}, loss:{loss.item():.4f}, {'succeed' if success else 'fail'}, steps:{len(rewards)}, total reward:{sum(rewards):.4f}, {t2-t1} sec\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train(verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54fb590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafbab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d19d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_entropy(N, alpha=1):\n",
    "    K = 2**N\n",
    "    values = np.arange(K)\n",
    "    unnormalized_probs = np.exp(-alpha * values)\n",
    "    Z = unnormalized_probs.sum()\n",
    "    probs = unnormalized_probs / Z\n",
    "    return values, -np.log2(probs)\n",
    "\n",
    "# Parameters\n",
    "N = 4  # Number of bits\n",
    "alpha = 1  # Decay rate\n",
    "\n",
    "# Compute probabilities\n",
    "values, probs = compute_entropy(N, alpha)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(values, probs, width=0.8, alpha=0.7, edgecolor=\"black\")\n",
    "plt.title(f\"Exponential Decay Probabilities (N={N}, alpha={alpha})\")\n",
    "plt.xlabel(\"Integer Value\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b95715bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minDelta for comparison 0: (1, 4, 0)\n",
      "minDelta for comparison 1: (1, 2, 0)\n",
      "minDelta for comparison 2: (1, 1, 0)\n",
      "minDelta for comparison 3: (2, 2, 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.7559647493809125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_min_delta(comparisons):\n",
    "    # Initialize the result list to store minDelta values\n",
    "    min_delta = []\n",
    "\n",
    "    # Iterate through each pair in the comparisons list\n",
    "    for i, (xi, yi) in enumerate(comparisons):\n",
    "        if i == 0:\n",
    "            # For i = 0, use the first case directly\n",
    "            min_delta.append((xi, min(yi, yi - xi), 0))\n",
    "        else:\n",
    "            # For i > 0, compute all possible options and select the minimal one\n",
    "            options = []\n",
    "\n",
    "            # Simple Entropy\n",
    "            simple_entropy = (xi, min(yi, yi - xi), 0)\n",
    "            options.append(simple_entropy)\n",
    "\n",
    "            # First Delta Entropy\n",
    "            xi_prev, yi_prev = comparisons[i - 1]\n",
    "            first_delta_entropy = (xi - xi_prev, yi - yi_prev, 0)\n",
    "            options.append(first_delta_entropy)\n",
    "\n",
    "            # Second Delta Entropy (only valid for i > 1)\n",
    "            if i > 1:\n",
    "                xi_prev2, yi_prev2 = comparisons[i - 2]\n",
    "                second_delta_entropy = (\n",
    "                    (xi - xi_prev) - (xi_prev - xi_prev2),\n",
    "                    (yi - yi_prev) - (yi_prev - yi_prev2),\n",
    "                    0,\n",
    "                )\n",
    "                options.append(second_delta_entropy)\n",
    "\n",
    "            # Arbitrary Position Entropy (only valid for i > 1)\n",
    "            for j in range(i):\n",
    "                xj, yj = comparisons[j]\n",
    "                arbitrary_position_entropy = (\n",
    "                    xi - xj,\n",
    "                    yi - yj,\n",
    "                    min(j, i - j),\n",
    "                )\n",
    "                options.append(arbitrary_position_entropy)\n",
    "\n",
    "            # Find the option with the minimal sum\n",
    "            min_delta.append(min(options, key=lambda t: sum([abs(x) for x in t])))\n",
    "\n",
    "    return min_delta\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "comparisons = [(1, 5), (2, 7), (4, 10), (8, 15)]\n",
    "result = compute_min_delta(comparisons)\n",
    "\n",
    "# Output the results\n",
    "for i, delta in enumerate(result):\n",
    "    print(f\"minDelta for comparison {i}: {delta}\")\n",
    "\n",
    "def compute_min_delta_entropy(comparisons):\n",
    "    # Initialize the result list to store minDelta values\n",
    "    min_delta = None\n",
    "\n",
    "    # Iterate through each pair in the comparisons list\n",
    "    i = len(comparisons) - 1\n",
    "    xi, yi = comparisons[i]\n",
    "    if i == 0:\n",
    "        # For i = 0, use the first case directly\n",
    "        min_delta = (xi, min(yi, yi - xi), 0)\n",
    "    else:\n",
    "        # For i > 0, compute all possible options and select the minimal one\n",
    "        options = []\n",
    "\n",
    "        # Simple Entropy\n",
    "        simple_entropy = (xi, min(yi, yi - xi), 0)\n",
    "        options.append(simple_entropy)\n",
    "\n",
    "        # First Delta Entropy\n",
    "        xi_prev, yi_prev = comparisons[i - 1]\n",
    "        first_delta_entropy = (xi - xi_prev, yi - yi_prev, 0)\n",
    "        options.append(first_delta_entropy)\n",
    "\n",
    "        # Second Delta Entropy (only valid for i > 1)\n",
    "        if i > 1:\n",
    "            xi_prev2, yi_prev2 = comparisons[i - 2]\n",
    "            second_delta_entropy = (\n",
    "                (xi - xi_prev) - (xi_prev - xi_prev2),\n",
    "                (yi - yi_prev) - (yi_prev - yi_prev2),\n",
    "                0,\n",
    "            )\n",
    "            options.append(second_delta_entropy)\n",
    "\n",
    "        # Arbitrary Position Entropy (only valid for i > 1)\n",
    "        for j in range(i):\n",
    "            xj, yj = comparisons[j]\n",
    "            arbitrary_position_entropy = (\n",
    "                xi - xj,\n",
    "                yi - yj,\n",
    "                min(j + 1, i - j),\n",
    "            )\n",
    "            options.append(arbitrary_position_entropy)\n",
    "\n",
    "        # Find the option with the minimal sum\n",
    "        min_delta = min(options, key=lambda t: sum([get_entropy_of_integer(x) for x in t]))\n",
    "\n",
    "    return sum([get_entropy_of_integer(x) for x in min_delta])\n",
    "\n",
    "compute_min_delta_entropy(comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1786443b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All cases succeeded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_case_1():\n",
    "    # Test with a single comparison\n",
    "    comparisons = [(3, 8)]\n",
    "    expected_result = [(3, 5, 0)]\n",
    "    assert compute_min_delta(comparisons) == expected_result\n",
    "\n",
    "def test_case_2():\n",
    "    # Test with multiple comparisons\n",
    "    comparisons = [(1, 5), (2, 7), (4, 10), (8, 15)]\n",
    "    expected_result = [\n",
    "        (1, 4, 0),  # Simple Entropy for the first pair\n",
    "        (1, 2, 0),  # Minimal sum using First Delta Entropy\n",
    "        (1, 1, 0),  # Minimal sum using First Delta Entropy\n",
    "        (2, 2, 0),  # Minimal sum using First Delta Entropy\n",
    "    ]\n",
    "    assert compute_min_delta(comparisons) == expected_result\n",
    "\n",
    "def test_case_3():\n",
    "    # Test with comparisons where the minimal sum comes from Arbitrary Position Entropy\n",
    "    comparisons = [(1, 10), (3, 12), (5, 14), (1, 11)]\n",
    "    expected_result = [\n",
    "        (1, 9, 0),  # Simple Entropy for the first pair\n",
    "        (2, 2, 0),  # Minimal sum using First Delta Entropy\n",
    "        (0, 0, 0),  # Minimal sum using First Delta Entropy\n",
    "        (0, 1, 0),  # Minimal sum using Arbitrary Position Entropy\n",
    "    ]\n",
    "    assert compute_min_delta(comparisons) == expected_result\n",
    "    \n",
    "def test_case_4():\n",
    "    # Test with comparisons where the minimal sum comes from Arbitrary Position Entropy\n",
    "    comparisons = [(1, 2), (2, 3), (3, 4), (4, 5), (1, 2)]\n",
    "    expected_result = [\n",
    "        (1, 1, 0),  # Simple Entropy for the first pair\n",
    "        (1, 1, 0),  # Minimal sum using First Delta Entropy\n",
    "        (0, 0, 0),  # Minimal sum using Second Delta Entropy\n",
    "        (0, 0, 0),  # Minimal sum using Second Delta Entropy\n",
    "        (0, 0, 0),  # Minimal sum using Arbitrary Position Entropy\n",
    "    ]\n",
    "    assert compute_min_delta(comparisons) == expected_result, compute_min_delta(comparisons)\n",
    "\n",
    "test_case_1()\n",
    "test_case_2()\n",
    "test_case_3()\n",
    "test_case_4()\n",
    "print(\"All cases succeeded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axiom",
   "language": "python",
   "name": "axiom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
