{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a8d7ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Episode 0: Success in 81 steps, reward: -5.30\n",
      "Episode 1: Success in 86 steps, reward: -0.90\n",
      "Episode 2: Success in 82 steps, reward: -3.60\n",
      "Episode 3: Success in 86 steps, reward: 1.00\n",
      "Episode 4: Success in 100 steps, reward: -7.10\n",
      "Episode 5: Success in 79 steps, reward: -1.10\n",
      "Episode 6: Success in 100 steps, reward: 4.00\n",
      "Episode 7: Success in 79 steps, reward: -3.40\n",
      "Episode 8: Success in 76 steps, reward: -3.10\n",
      "Episode 9: Success in 77 steps, reward: -2.40\n",
      "Episode 10: Success in 101 steps, reward: -2.20\n",
      "Episode 11: Success in 91 steps, reward: -6.30\n",
      "Episode 12: Success in 100 steps, reward: 0.40\n",
      "Episode 13: Success in 79 steps, reward: -5.00\n",
      "Episode 14: Success in 83 steps, reward: 0.30\n",
      "Episode 15: Success in 76 steps, reward: -3.80\n",
      "Episode 16: Success in 80 steps, reward: -3.00\n",
      "Episode 17: Success in 93 steps, reward: 2.20\n",
      "Episode 18: Success in 99 steps, reward: -0.60\n",
      "Episode 19: Success in 88 steps, reward: 1.10\n",
      "Episode 20: Success in 97 steps, reward: -1.90\n",
      "Episode 21: Success in 97 steps, reward: -3.90\n",
      "Episode 22: Success in 97 steps, reward: -5.80\n",
      "Episode 23: Success in 85 steps, reward: -2.60\n",
      "Episode 24: Success in 112 steps, reward: -17.20\n",
      "Episode 25: Success in 74 steps, reward: -4.80\n",
      "Episode 26: Success in 74 steps, reward: -4.80\n",
      "Episode 27: Success in 95 steps, reward: 3.20\n",
      "Episode 28: Success in 86 steps, reward: 1.40\n",
      "Episode 29: Success in 86 steps, reward: -5.10\n",
      "Episode 30: Success in 96 steps, reward: -4.60\n",
      "Episode 31: Success in 92 steps, reward: -0.10\n",
      "Episode 32: Success in 80 steps, reward: -9.20\n",
      "Episode 33: Success in 80 steps, reward: -1.40\n",
      "Episode 34: Success in 83 steps, reward: 1.90\n",
      "Episode 35: Success in 91 steps, reward: 1.80\n",
      "Episode 36: Success in 92 steps, reward: 3.80\n",
      "Episode 37: Success in 100 steps, reward: 3.00\n",
      "Episode 38: Success in 87 steps, reward: -0.90\n",
      "Episode 39: Success in 83 steps, reward: -0.70\n",
      "Episode 40: Success in 75 steps, reward: -1.20\n",
      "Episode 41: Success in 109 steps, reward: -3.60\n",
      "Episode 42: Success in 107 steps, reward: 1.90\n",
      "Episode 43: Success in 84 steps, reward: -3.20\n",
      "Episode 44: Success in 86 steps, reward: 3.30\n",
      "Episode 45: Success in 82 steps, reward: -3.00\n",
      "Episode 46: Success in 98 steps, reward: 4.90\n",
      "Episode 47: Success in 85 steps, reward: -2.90\n",
      "Episode 48: Success in 84 steps, reward: -1.30\n",
      "Episode 49: Success in 75 steps, reward: -2.20\n",
      "Episode 50: Success in 96 steps, reward: 4.20\n",
      "Episode 51: Success in 91 steps, reward: 1.50\n",
      "Episode 52: Success in 78 steps, reward: -5.30\n",
      "Episode 53: Success in 72 steps, reward: -2.90\n",
      "Episode 54: Success in 96 steps, reward: 5.80\n",
      "Episode 55: Success in 79 steps, reward: -3.90\n",
      "Episode 56: Success in 98 steps, reward: -5.50\n",
      "Episode 57: Success in 82 steps, reward: 0.90\n",
      "Episode 58: Success in 77 steps, reward: -3.10\n",
      "Episode 59: Success in 85 steps, reward: 2.60\n",
      "Episode 60: Success in 101 steps, reward: -3.50\n",
      "Episode 61: Success in 86 steps, reward: -1.20\n",
      "Episode 62: Success in 97 steps, reward: -5.20\n",
      "Episode 63: Success in 115 steps, reward: 3.70\n",
      "Episode 64: Success in 99 steps, reward: 1.00\n",
      "Episode 65: Success in 88 steps, reward: -2.20\n",
      "Episode 66: Success in 92 steps, reward: -6.30\n",
      "Episode 67: Success in 96 steps, reward: 2.90\n",
      "Episode 68: Success in 85 steps, reward: -4.20\n",
      "Episode 69: Success in 100 steps, reward: -9.70\n",
      "Episode 70: Success in 88 steps, reward: 3.00\n",
      "Episode 71: Success in 96 steps, reward: -5.20\n",
      "Episode 72: Success in 100 steps, reward: -4.80\n",
      "Episode 73: Success in 101 steps, reward: -10.60\n",
      "Episode 74: Success in 89 steps, reward: -5.40\n",
      "Episode 75: Success in 86 steps, reward: 2.00\n",
      "Episode 76: Success in 86 steps, reward: -0.60\n",
      "Episode 77: Success in 98 steps, reward: -1.60\n",
      "Episode 78: Success in 97 steps, reward: 1.30\n",
      "Episode 79: Success in 86 steps, reward: -1.90\n",
      "Episode 80: Success in 102 steps, reward: 4.00\n",
      "Episode 81: Success in 91 steps, reward: 0.50\n",
      "Episode 82: Success in 84 steps, reward: -4.90\n",
      "Episode 83: Success in 94 steps, reward: 0.90\n",
      "Episode 84: Success in 81 steps, reward: 0.60\n",
      "Episode 85: Success in 102 steps, reward: -9.30\n",
      "Episode 86: Success in 107 steps, reward: -3.30\n",
      "Episode 87: Success in 82 steps, reward: 0.60\n",
      "Episode 88: Success in 96 steps, reward: 2.90\n",
      "Episode 89: Success in 86 steps, reward: 0.70\n",
      "Episode 90: Success in 92 steps, reward: -2.10\n",
      "Episode 91: Success in 91 steps, reward: -7.30\n",
      "Episode 92: Success in 76 steps, reward: -7.00\n",
      "Episode 93: Success in 83 steps, reward: 0.30\n",
      "Episode 94: Success in 100 steps, reward: 4.60\n",
      "Episode 95: Success in 80 steps, reward: -1.10\n",
      "Episode 96: Success in 100 steps, reward: 5.90\n",
      "Episode 97: Success in 85 steps, reward: -2.20\n",
      "Episode 98: Success in 88 steps, reward: -2.20\n",
      "Episode 99: Success in 101 steps, reward: -12.20\n",
      "Success rate: 100.00%\n",
      "Average steps per episode: 89.69\n",
      "Average reward per episode: -1.79\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import math\n",
    "import random\n",
    "\n",
    "MIN_LIST_LEN = 16\n",
    "MAX_LIST_LEN = 16\n",
    "MAX_STEPS = 640\n",
    "\n",
    "SUCCESS_REWARD = 0.5\n",
    "STEP_REWARD = -0.3\n",
    "COMPARISON_ENTROPY_MULTIPLIER = -0.00\n",
    "SWAP_REWARD = 1.0\n",
    "INVALID_ACTION_REWARD = -10.0\n",
    "LONGTERM_GAMMA = 0.99\n",
    "SHORTTERM_GAMMA = 0.7\n",
    "\n",
    "EPS_START = 0.5\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "LR_SCHEDULER_GAMMA = 0.96\n",
    "NUM_EPISODES = 200000\n",
    "EPISODES_SAVE = 1000\n",
    "OUTPUT_DIR = '/home/mcwave/code/autocode/datasets/rl_sort_transformer_easy/list16_transformer4_192_gamma07_step640_v3'\n",
    "\n",
    "# Define the vocabulary\n",
    "vocab = {\n",
    "    'Comparison': 0,\n",
    "    'Swap': 1,\n",
    "    'less': 2,\n",
    "    'equal': 3,\n",
    "    'more': 4,\n",
    "    '0': 5,\n",
    "    '1': 6,\n",
    "    '2': 7,\n",
    "    '3': 8,\n",
    "    '4': 9,\n",
    "    '5': 10,\n",
    "    '6': 11,\n",
    "    '7': 12,\n",
    "    '8': 13,\n",
    "    '9': 14,\n",
    "    '10': 15,\n",
    "    '11': 16,\n",
    "    '12': 17,\n",
    "    '13': 18,\n",
    "    '14': 19,\n",
    "    '15': 20,\n",
    "    'len1': 21,\n",
    "    'len2': 22,\n",
    "    'len3': 23,\n",
    "    'len4': 24,\n",
    "    'len5': 25,\n",
    "    'len6': 26,\n",
    "    'len7': 27,\n",
    "    'len8': 28,\n",
    "    'len9': 29,\n",
    "    'len10': 30,\n",
    "    'len11': 31,\n",
    "    'len12': 32,\n",
    "    'len13': 33,\n",
    "    'len14': 34,\n",
    "    'len15': 35,\n",
    "    'len16': 36,\n",
    "}\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "def compute_entropy(N, alpha=1):\n",
    "    K = 2**N\n",
    "    values = np.arange(K)\n",
    "    unnormalized_probs = np.exp(-alpha * values)\n",
    "    Z = unnormalized_probs.sum()\n",
    "    probs = unnormalized_probs / Z\n",
    "    return values, -np.log2(probs)\n",
    "\n",
    "_, int_entropy = compute_entropy(4)\n",
    "\n",
    "def get_entropy_of_integer(x):\n",
    "    x = min(15, abs(x))\n",
    "    return int_entropy[x]\n",
    "\n",
    "def compute_min_delta_entropy(comparisons):\n",
    "    # Initialize the result list to store minDelta values\n",
    "    min_delta = None\n",
    "\n",
    "    # Iterate through each pair in the comparisons list\n",
    "    i = len(comparisons) - 1\n",
    "    xi, yi = comparisons[i]\n",
    "    if i == 0:\n",
    "        # For i = 0, use the first case directly\n",
    "        min_delta = (xi, min(yi, yi - xi), 0)\n",
    "    else:\n",
    "        # For i > 0, compute all possible options and select the minimal one\n",
    "        options = []\n",
    "\n",
    "        # Simple Entropy\n",
    "        simple_entropy = (xi, min(yi, yi - xi), 0)\n",
    "        options.append(simple_entropy)\n",
    "\n",
    "        # First Delta Entropy\n",
    "        xi_prev, yi_prev = comparisons[i - 1]\n",
    "        first_delta_entropy = (xi - xi_prev, yi - yi_prev, 0)\n",
    "        options.append(first_delta_entropy)\n",
    "\n",
    "        # Second Delta Entropy (only valid for i > 1)\n",
    "        if i > 1:\n",
    "            xi_prev2, yi_prev2 = comparisons[i - 2]\n",
    "            second_delta_entropy = (\n",
    "                (xi - xi_prev) - (xi_prev - xi_prev2),\n",
    "                (yi - yi_prev) - (yi_prev - yi_prev2),\n",
    "                0,\n",
    "            )\n",
    "            options.append(second_delta_entropy)\n",
    "\n",
    "        # Arbitrary Position Entropy (only valid for i > 1)\n",
    "        for j in range(i):\n",
    "            xj, yj = comparisons[j]\n",
    "            arbitrary_position_entropy = (\n",
    "                xi - xj,\n",
    "                yi - yj,\n",
    "                min(j, i - j),\n",
    "            )\n",
    "            options.append(arbitrary_position_entropy)\n",
    "\n",
    "        # Find the option with the minimal sum\n",
    "        min_delta = min(options, key=lambda t: sum([get_entropy_of_integer(x) for x in t]))\n",
    "\n",
    "    entropy = sum([get_entropy_of_integer(x) for x in min_delta])\n",
    "    if len(comparisons) == 1:\n",
    "        return 3 * entropy\n",
    "    else:\n",
    "        return entropy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the environment\n",
    "class SortingEnv:\n",
    "    def __init__(self):\n",
    "        self.max_steps = MAX_STEPS\n",
    "\n",
    "    def reset(self):\n",
    "        self.length = random.randint(MIN_LIST_LEN, MAX_LIST_LEN)\n",
    "        self.list = [random.randint(1, 100) for _ in range(self.length)]\n",
    "        while self.list == sorted(self.list):\n",
    "            self.list = [random.randint(1, 100) for _ in range(self.length)]\n",
    "        self.indices = None\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        initial_token = 'len{}'.format(self.length)\n",
    "        return vocab[initial_token], self.list.copy()\n",
    "    \n",
    "    def get_list(self):\n",
    "        return self.list\n",
    "    \n",
    "    def get_list_len(self):\n",
    "        return len(self.list)\n",
    "\n",
    "    def step(self, action_tokens):\n",
    "        action = action_tokens[0]\n",
    "        reward = -0.01  # default penalty\n",
    "        response_token = None\n",
    "\n",
    "        if action == vocab['Comparison']:\n",
    "            if len(action_tokens) != 3:\n",
    "                reward = INVALID_ACTION_REWARD\n",
    "                self.done = True\n",
    "                return response_token, reward, self.done, self.list.copy()\n",
    "            index1 = action_tokens[1] - vocab['0']\n",
    "            index2 = action_tokens[2] - vocab['0']\n",
    "            if index1 >= self.length or index2 >= self.length or index1 < 0 or index2 < 0:\n",
    "                reward = INVALID_ACTION_REWARD\n",
    "                self.done = True\n",
    "                return response_token, reward, self.done, self.list.copy()\n",
    "            self.indices = (index1, index2)\n",
    "            if self.list[index1] < self.list[index2]:\n",
    "                response_token = vocab['less']\n",
    "                reward = STEP_REWARD\n",
    "            elif self.list[index1] == self.list[index2]:\n",
    "                response_token = vocab['equal']\n",
    "                reward = STEP_REWARD * 2\n",
    "            else:\n",
    "                response_token = vocab['more']\n",
    "                reward = STEP_REWARD\n",
    "        elif action == vocab['Swap']:\n",
    "            if self.indices is None:\n",
    "                reward = INVALID_ACTION_REWARD\n",
    "                self.done = True\n",
    "                return response_token, reward, self.done, self.list.copy()\n",
    "            index1, index2 = self.indices\n",
    "            prev_list = self.list.copy()\n",
    "            self.list[index1], self.list[index2] = self.list[index2], self.list[index1]\n",
    "            if self.list == sorted(self.list):\n",
    "                reward = SUCCESS_REWARD\n",
    "                self.done = True\n",
    "            elif (index1 < index2 and prev_list[index1] > prev_list[index2] and self.list[index1] <= self.list[index2]) or \\\n",
    "                (index1 > index2 and prev_list[index1] < prev_list[index2] and self.list[index1] >= self.list[index2]):\n",
    "                reward = SWAP_REWARD\n",
    "            elif (index1 < index2 and prev_list[index1] < prev_list[index2] and self.list[index1] >= self.list[index2]) or \\\n",
    "                (index1 > index2 and prev_list[index1] > prev_list[index2] and self.list[index1] <= self.list[index2]):\n",
    "                reward = -SWAP_REWARD\n",
    "            else:\n",
    "                reward = STEP_REWARD\n",
    "            self.indices = None\n",
    "        else:\n",
    "            reward = INVALID_ACTION_REWARD\n",
    "            self.done = True\n",
    "\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            self.done = True\n",
    "        return response_token, reward, self.done, self.list.copy()\n",
    "\n",
    "def decode(input_tokens, inv_vocab):\n",
    "    return ' '.join([inv_vocab[x] for x in input_tokens])\n",
    "\n",
    "# Quicksort Algorithm using the environment\n",
    "def run_quicksort_episode(verbose=False):\n",
    "    env = SortingEnv()\n",
    "    initial_token_id, current_list = env.reset()\n",
    "    done = False\n",
    "    total_steps = 0\n",
    "    total_reward = 0.0\n",
    "    success = False\n",
    "    length = env.length\n",
    "\n",
    "    def quicksort(env, low, high):\n",
    "        if env.done:\n",
    "            return\n",
    "        if low < high:\n",
    "            pi = partition(env, low, high)\n",
    "            if pi is None:\n",
    "                return\n",
    "            quicksort(env, low, pi - 1)\n",
    "            quicksort(env, pi + 1, high)\n",
    "\n",
    "    def partition(env, low, high):\n",
    "        nonlocal total_reward, total_steps  # Declare nonlocal variables\n",
    "        if env.done:\n",
    "            return None\n",
    "        pivot_index = high\n",
    "        i = low - 1\n",
    "        for j in range(low, high):\n",
    "            # Compare arr[j] with arr[pivot_index]\n",
    "            action_tokens = [vocab['Comparison'], vocab[str(j)], vocab[str(pivot_index)]]\n",
    "            response_token, reward, done, current_list = env.step(action_tokens)\n",
    "            if verbose:\n",
    "                print(f\"Comparison between indices {j} and {pivot_index}, result: {inv_vocab[response_token]}\")\n",
    "            total_reward += reward\n",
    "            total_steps += 1\n",
    "            if done:\n",
    "                return None\n",
    "            if response_token == vocab['less'] or response_token == vocab['equal']:\n",
    "                i += 1\n",
    "                if i != j:\n",
    "                    # Need to swap arr[i] and arr[j]\n",
    "                    # Perform 'Comparison' to set indices\n",
    "                    action_tokens = [vocab['Comparison'], vocab[str(i)], vocab[str(j)]]\n",
    "                    response_token, reward, done, current_list = env.step(action_tokens)\n",
    "                    if verbose:\n",
    "                        print(f\"Comparison between indices {i} and {j} for swap setup, result: {inv_vocab[response_token]}\")\n",
    "                    total_reward += reward\n",
    "                    total_steps += 1\n",
    "                    if done:\n",
    "                        return None\n",
    "                    # Perform 'Swap'\n",
    "                    action_tokens = [vocab['Swap']]\n",
    "                    response_token, reward, done, current_list = env.step(action_tokens)\n",
    "                    if verbose:\n",
    "                        print(f\"Swapped indices {i} and {j}\")\n",
    "                    total_reward += reward\n",
    "                    total_steps += 1\n",
    "                    if done:\n",
    "                        return None\n",
    "        # Swap arr[i+1] and arr[high] (pivot)\n",
    "        if i + 1 != pivot_index:\n",
    "            # Perform 'Comparison' to set indices\n",
    "            action_tokens = [vocab['Comparison'], vocab[str(i + 1)], vocab[str(pivot_index)]]\n",
    "            response_token, reward, done, current_list = env.step(action_tokens)\n",
    "            if verbose:\n",
    "                print(f\"Comparison between indices {i + 1} and {pivot_index} for final swap, result: {inv_vocab[response_token]}\")\n",
    "            total_reward += reward\n",
    "            total_steps += 1\n",
    "            if done:\n",
    "                return None\n",
    "            # Perform 'Swap'\n",
    "            action_tokens = [vocab['Swap']]\n",
    "            response_token, reward, done, current_list = env.step(action_tokens)\n",
    "            if verbose:\n",
    "                print(f\"Swapped indices {i + 1} and {pivot_index}\")\n",
    "            total_reward += reward\n",
    "            total_steps += 1\n",
    "            if done:\n",
    "                return None\n",
    "        return i + 1\n",
    "\n",
    "    quicksort(env, 0, length - 1)\n",
    "\n",
    "    if env.list == sorted(env.list):\n",
    "        success = True\n",
    "    else:\n",
    "        success = False\n",
    "\n",
    "    return success, total_steps, total_reward, env.list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    NUM_EPISODES = 100\n",
    "    total_successes = 0\n",
    "    total_steps = 0\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        success, steps, reward, sorted_list = run_quicksort_episode(verbose=False)\n",
    "        total_steps += steps\n",
    "        total_reward += reward\n",
    "        if success:\n",
    "            total_successes += 1\n",
    "            print(f\"Episode {episode}: Success in {steps} steps, reward: {reward:.2f}\")\n",
    "        else:\n",
    "            print(f\"Episode {episode}: Fail in {steps} steps, reward: {reward:.2f}\")\n",
    "\n",
    "    print(f\"Success rate: {total_successes / NUM_EPISODES * 100:.2f}%\")\n",
    "    print(f\"Average steps per episode: {total_steps / NUM_EPISODES:.2f}\")\n",
    "    print(f\"Average reward per episode: {total_reward / NUM_EPISODES:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axiom",
   "language": "python",
   "name": "axiom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
